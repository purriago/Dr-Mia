{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BItY4myhkWR3",
        "outputId": "a63379f1-2eac-4972-f8b5-bc27a1575f7e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 282 images belonging to 2 classes.\n",
            "Found 70 images belonging to 2 classes.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 4s/step - accuracy: 0.5587 - loss: 0.7635 - val_accuracy: 0.6714 - val_loss: 0.5997\n",
            "Epoch 2/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.6616 - loss: 0.6148 - val_accuracy: 0.6714 - val_loss: 0.5582\n",
            "Epoch 3/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.6754 - loss: 0.5642 - val_accuracy: 0.6714 - val_loss: 0.5230\n",
            "Epoch 4/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.6540 - loss: 0.5972 - val_accuracy: 0.6714 - val_loss: 0.8306\n",
            "Epoch 5/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.6804 - loss: 0.6704 - val_accuracy: 0.6714 - val_loss: 0.7926\n",
            "Epoch 6/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 0.7382 - loss: 0.5700 - val_accuracy: 0.6714 - val_loss: 0.4982\n",
            "Epoch 7/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.7693 - loss: 0.4362 - val_accuracy: 0.7571 - val_loss: 0.4353\n",
            "Epoch 8/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.8432 - loss: 0.4127 - val_accuracy: 0.6857 - val_loss: 0.7293\n",
            "Epoch 9/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.7980 - loss: 0.4359 - val_accuracy: 0.6571 - val_loss: 0.6730\n",
            "Epoch 10/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 0.8767 - loss: 0.3441 - val_accuracy: 0.8000 - val_loss: 0.4325\n",
            "Epoch 11/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.8664 - loss: 0.3190 - val_accuracy: 0.8000 - val_loss: 0.4309\n",
            "Epoch 12/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.8803 - loss: 0.3163 - val_accuracy: 0.7857 - val_loss: 0.4636\n",
            "Epoch 13/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 0.8528 - loss: 0.3567 - val_accuracy: 0.6571 - val_loss: 0.5962\n",
            "Epoch 14/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.8821 - loss: 0.2406 - val_accuracy: 0.8429 - val_loss: 0.4364\n",
            "Epoch 15/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.8464 - loss: 0.2954 - val_accuracy: 0.7143 - val_loss: 0.7064\n",
            "Epoch 16/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 0.9303 - loss: 0.1723 - val_accuracy: 0.7857 - val_loss: 0.5610\n",
            "Epoch 17/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.9382 - loss: 0.1781 - val_accuracy: 0.8000 - val_loss: 0.5838\n",
            "Epoch 18/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.9496 - loss: 0.1537 - val_accuracy: 0.6857 - val_loss: 0.8042\n",
            "Epoch 19/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.8761 - loss: 0.2496 - val_accuracy: 0.7143 - val_loss: 0.6994\n",
            "Epoch 20/20\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.8802 - loss: 0.2720 - val_accuracy: 0.8571 - val_loss: 0.6245\n"
          ]
        }
      ],
      "source": [
        "#Librerias necesarias de Tensorflow y Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "#libreria para montar la carpeta de google drive que contiene las muestras\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "data_dir = \"/content/drive/MyDrive/Microplasticos\"\n",
        "# Definimos el tamaño al que se redimensionarán todas las imágenes (ancho x alto)\n",
        "img_height, img_width = 128, 128\n",
        "\n",
        "# Número de imágenes que se procesarán en cada lote\n",
        "batch_size = 32\n",
        "\n",
        "#Se cargan y preparan los datos\n",
        "#Separa automáticamente el conjunto en entrenamiento (80%) y validación (20%)\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,                   #Reescala las imágenes dividiendo los valores de los píxeles entre 255 (normaliza los datos a [0, 1]).\n",
        "    validation_split=0.2)             # 80% entrenamiento, 20% validación\n",
        "\n",
        "# Cargamos las imágenes para entrenamiento desde una carpeta organizada por clases\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,                  # Ruta a la carpeta raíz con subcarpetas por clase\n",
        "    target_size=(img_height, img_width),    # Redimensionamos todas las imágenes a 128x128\n",
        "    batch_size=batch_size,                  # Tamaño del lote\n",
        "    class_mode='binary',                    # Clasificación binaria (0 o 1)\n",
        "    subset='training'                       # Este generador es para entrenamiento\n",
        ")\n",
        "\n",
        "# Cargamos las imágenes para validación\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,                  # Ruta a la carpeta raíz con subcarpetas por clase\n",
        "    target_size=(img_height, img_width),    # Redimensionamos todas las imágenes a 128x128\n",
        "    batch_size=batch_size,                  # Tamaño del lote\n",
        "    class_mode='binary',                    # Clasificación binaria (0 o 1)\n",
        "    subset='validation'                     # Este generador es para validación\n",
        ")\n",
        "\n",
        "# Definimos el modelo secuencial (una capa tras otra)\n",
        "model = Sequential([\n",
        "    #Primera capa convolucional: 32 filtros de 3x3, activación ReLU\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "    MaxPooling2D(pool_size=(2,2)),          # Reducción de tamaño de la imagen\n",
        "\n",
        "    #Segunda capa convolucional: 64 filtros\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    #Tercera capa convolucional: 128 filtros\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    # Aplanamos las salidas 2D para convertirlas en un vector 1D\n",
        "    Flatten(),\n",
        "\n",
        "     # Capa completamente conectada con 128 neuronas\n",
        "    Dense(128, activation='relu'),\n",
        "\n",
        "    # Dropout para reducir el sobreajuste (desactiva aleatoriamente el 50% de las neuronas)\n",
        "    Dropout(0.5),\n",
        "\n",
        "    #Capa de salida: 1 neurona con activación sigmoide (da una probabilidad)\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compilamos el modelo indicando el optimizador, la función de pérdida y la métrica de evaluación\n",
        "model.compile(optimizer='adam',             #Optimizador eficiente y adaptativo\n",
        "              loss='binary_crossentropy',   #Función de pérdida para clasificación binaria\n",
        "              metrics=['accuracy'])         #Métrica de precisión\n",
        "\n",
        "#Se entrena el modelo con los datos de entrenamiento y validación\n",
        "model.fit(\n",
        "    train_generator,                        #Datos de entrenamiento\n",
        "    epochs=20,                              #Número de veces que se repite el entrenamiento\n",
        "    validation_data=validation_generator    # Datos de validación\n",
        ")\n",
        "\n",
        "# Guardar el modelo entrenado\n",
        "model.save('modelo_microplasticos.keras')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model = load_model(\"/content/modelo_microplasticos.keras\")\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Microplasticos'\n",
        "img_height, img_width = 128, 128\n",
        "batch_size = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Cargamos las imágenes para entrenamiento desde una carpeta organizada por clases\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,                  # Ruta a la carpeta raíz con subcarpetas por clase\n",
        "    target_size=(img_height, img_width),    # Redimensionamos todas las imágenes a 128x128\n",
        "    batch_size=batch_size,                  # Tamaño del lote\n",
        "    class_mode='binary',                    # Clasificación binaria (0 o 1)\n",
        "    subset='training'                       # Este generador es para entrenamiento\n",
        ")\n",
        "\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,                  # Ruta a la carpeta raíz con subcarpetas por clase\n",
        "    target_size=(img_height, img_width),    # Redimensionamos todas las imágenes a 128x128\n",
        "    batch_size=batch_size,                  # Tamaño del lote\n",
        "    class_mode='binary',                    # Clasificación binaria (0 o 1)\n",
        "    subset='validation'                     # Este generador es para validación\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.save('modelo_microplasticos_v2.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50zaiLC63xrx",
        "outputId": "7230101a-e3c1-4583-d669-71e7e4b55344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 282 images belonging to 2 classes.\n",
            "Found 70 images belonging to 2 classes.\n",
            "Epoch 1/30\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.7785 - loss: 0.5579 - val_accuracy: 0.7571 - val_loss: 0.6852\n",
            "Epoch 2/30\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.7801 - loss: 0.7016 - val_accuracy: 0.7571 - val_loss: 0.5973\n",
            "Epoch 3/30\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.8015 - loss: 0.5399 - val_accuracy: 0.8143 - val_loss: 0.5355\n",
            "Epoch 4/30\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.8577 - loss: 0.3479 - val_accuracy: 0.7000 - val_loss: 0.5768\n",
            "Epoch 5/30\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.8462 - loss: 0.4131 - val_accuracy: 0.7714 - val_loss: 0.5603\n",
            "Epoch 6/30\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.8310 - loss: 0.3933 - val_accuracy: 0.8000 - val_loss: 0.7138\n",
            "Epoch 7/30\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.8464 - loss: 0.5024 - val_accuracy: 0.7429 - val_loss: 0.4024\n",
            "Epoch 8/30\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.8486 - loss: 0.4936 - val_accuracy: 0.7571 - val_loss: 0.5051\n",
            "Epoch 9/30\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.8407 - loss: 0.4829 - val_accuracy: 0.7857 - val_loss: 0.4983\n",
            "Epoch 10/30\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.8301 - loss: 0.4256 - val_accuracy: 0.7714 - val_loss: 0.5015\n",
            "Epoch 11/30\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.8269 - loss: 0.4308 - val_accuracy: 0.7571 - val_loss: 0.4513\n",
            "Epoch 12/30\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.8463 - loss: 0.3472 - val_accuracy: 0.7714 - val_loss: 0.5548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/Microplasticos/modelo_microplasticos_v2.keras')"
      ],
      "metadata": {
        "id": "TOx8qyUPa__m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. MONTAR GOOGLE DRIVE\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Carga la imagen\n",
        "img_path = '/content/20250715-120701-641.jpg'\n",
        "model = load_model('/content/drive/MyDrive/Microplasticos/modelo_microplasticos_v2.keras')\n",
        "# Convierte a array y normaliza\n",
        "  # Para batch de 1 imagen\n",
        "\n",
        "img = image.load_img(img_path, target_size=(128, 128))\n",
        "img_array = image.img_to_array(img) / 255.0\n",
        "img_array = np.expand_dims(img_array, axis=0)  # batch size = 1\n",
        "# Predicción\n",
        "prediction = model.predict(img_array)\n",
        "\n",
        "# Resultado\n",
        "if prediction[0][0] >= 0.5:\n",
        "    print(\" Microplástico detectado\")\n",
        "else:\n",
        "    print(\" No hay microplástico\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "ru9SYQsmbdUi",
        "outputId": "fb6806a5-5c59-4e24-f8b7-2eb82f82e2c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/20250715-120701-641.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3430879695.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# Para batch de 1 imagen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# batch size = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/20250715-120701-641.jpg'"
          ]
        }
      ]
    }
  ]
}